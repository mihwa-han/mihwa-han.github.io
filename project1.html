
<!DOCTYPE html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
	<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>CNN Deep Learning &mdash; Free Fully Responsive HTML5 Bootstrap Template by FREEHTML5.co</title>
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="Free HTML5 Template by FREEHTML5.CO" />
	<meta name="keywords" content="free html5, free template, free bootstrap, html5, css3, mobile first, responsive" />
	<meta name="author" content="FREEHTML5.CO" />

  <!--
	//////////////////////////////////////////////////////

	FREE HTML5 TEMPLATE
	DESIGNED & DEVELOPED by FREEHTML5.CO

	Website: 		http://freehtml5.co/
	Email: 			info@freehtml5.co
	Twitter: 		http://twitter.com/fh5co
	Facebook: 		https://www.facebook.com/fh5co

	//////////////////////////////////////////////////////
	 -->

  	<!-- Facebook and Twitter integration -->
	<meta property="og:title" content=""/>
	<meta property="og:image" content=""/>
	<meta property="og:url" content=""/>
	<meta property="og:site_name" content=""/>
	<meta property="og:description" content=""/>
	<meta name="twitter:title" content="" />
	<meta name="twitter:image" content="" />
	<meta name="twitter:url" content="" />
	<meta name="twitter:card" content="" />

	<!-- Place favicon.ico and apple-touch-icon.png in the root directory -->
	<link rel="shortcut icon" href="favicon.ico">
	<!-- Google Fonts -->
	<link href='http://fonts.googleapis.com/css?family=Playfair+Display:400,700,400italic|Roboto:400,300,700' rel='stylesheet' type='text/css'>
	<!-- Animate -->
	<link rel="stylesheet" href="css/animate.css">
	<!-- Icomoon -->
	<link rel="stylesheet" href="css/icomoon.css">
	<!-- Bootstrap  -->
	<link rel="stylesheet" href="css/bootstrap.css">

	<link rel="stylesheet" href="css/style.css">


	<!-- Modernizr JS -->
	<script src="js/modernizr-2.6.2.min.js"></script>
	<!-- FOR IE9 below -->
	<!--[if lt IE 9]>
	<script src="js/respond.min.js"></script>
	<![endif]-->

	</head>
	<body>
	<div id="fh5co-offcanvas">
		<a href="#" class="fh5co-close-offcanvas js-fh5co-close-offcanvas"><span><i class="icon-cross3"></i> <span>Close</span></span></a>
		<div class="fh5co-bio">
			<figure>
				<img src="images/person1.jpg" alt="Free HTML5 Bootstrap Template" class="img-responsive">
			</figure>
			<h3 class="heading">About Me</h3>
			<h2>Mihwa Han</h2>
			<p>Data Lover</p>
			<ul class="fh5co-social">
				<li><a href="https://github.com/mihwa-han"><i class="icon-github"></i></a></li>
				<li><a href="https://www.linkedin.com/in/mihwa-han"><i class="icon-linkedin"></i></a></li>
				<!--
				<li><a href="#"><i class="icon-twitter"></i></a></li>
				<li><a href="#"><i class="icon-facebook"></i></a></li>
				<li><a href="#"><i class="icon-instagram"></i></a></li>
			-->
			</ul>
		</div>

		<div class="fh5co-menu">
			<div class="fh5co-box">
				<h3 class="heading">Categories</h3>
				<ul>
					<li><a href="#">Data Science</a></li>
					<li><a href="#">My Story</a></li>
					<li><a href="#">Photography</a></li>
					<li><a href="#">Boston</a></li>
				</ul>
			</div>
			<div class="fh5co-box">
				<h3 class="heading">Search</h3>
				<form action="#">
					<div class="form-group">
						<input type="text" class="form-control" placeholder="Type a keyword">
					</div>
				</form>
			</div>
		</div>
	</div>

	<!-- END #fh5co-offcanvas -->
	<header id="fh5co-header">

		<div class="container-fluid">

			<div class="row">
				<a href="#" class="js-fh5co-nav-toggle fh5co-nav-toggle"><i></i></a>
				<ul class="fh5co-social">
					<li><a href="https://github.com/mihwa-han"><i class="icon-github"></i></a></li>
					<li><a href="https://www.linkedin.com/in/mihwa-han/"><i class="icon-linkedin"></i></a></li>
					<!--
					<li><a href="#"><i class="icon-twitter"></i></a></li>
					<li><a href="#"><i class="icon-facebook"></i></a></li>
					<li><a href="#"><i class="icon-instagram"></i></a></li>
				-->
				</ul>
				<!--
				<div class="col-lg-12 col-md-12 text-center">
					<h1 id="fh5co-logo"><a href="index.html">Data Science Magazine</a></h1>
				</div>
-->
			</div>

		</div>

	</header>
	<!--
	<a href="#" class="fh5co-post-prev"><span><i class="icon-chevron-left"></i> Prev</span></a>
	<a href="#" class="fh5co-post-next"><span>Next <i class="icon-chevron-right"></i></span></a>
-->
	<!-- END #fh5co-header -->
	<div class="container-fluid">
		<div class="row fh5co-post-entry single-entry">
			<article class="col-lg-8 col-lg-offset-2 col-md-8 col-md-offset-2 col-sm-8 col-sm-offset-2 col-xs-12 col-xs-offset-0">
				<center><figure class="animate-box">
					<img src="images/dog-breeds.jpg" alt="Image" class="img-responsive">
				</figure></center>
				<span class="fh5co-meta animate-box"><a href="single.html">CNN</a></span>
				<h2 class="fh5co-article-title animate-box"><a href="single.html">Dog Breed Classification using Deep Learning</a></h2>
				<span class="fh5co-meta fh5co-date animate-box">December 22th, 2017</span>

				<div class="col-lg-12 col-lg-offset-0 col-md-12 col-md-offset-0 col-sm-12 col-sm-offset-0 col-xs-12 col-xs-offset-0 text-left content-article">
					<div class="row">
						<div class="col-lg-8 cp-r animate-box">
							<p>Pets occupy a very special place in our households; in most cases they are considered a member of the family.
Every time I see a new dog I wonder what breed that dog is.
There are more than 300 unique dog breeds in the world, and many dogs are mixed with multiple breeds.
As a result, it is often very difficult to know the breed of a dog.
But, can we accurately determine a dog's breed using machine learning?</p>


							<p>In many cases, this will be a very difficult challenge because even a trained human
								 would have a hard time distinguishing between some breeds of dogs from an image.
								 For example, the Brittany, Irish Red & White Setter, and Welsh Springer
								  Spaniel look very much alike. Another example of breeds that show only slight
									differences are American Water Spaniel and Curly-Coated Retriever.
									To add to the potential for confusion, there are many cases of dogs
									within the same breed that show fairly significant variations (for example,
									 see the American Staffordshire Terrier below).</p>


							<p> In this post, I will demonstrate how to use deep learning to try to automatically
								identify dog breeds from images.  This project was inspired by the Udacity
								Machine Learning Nanodegree program I participated in. </p>

			</div>
						<div class="col-lg-4 animate-box">
							<div class="fh5co-highlight right">
								<center><figure>
									<img src="images/Brittany_02625.jpg" width='160', height='160'>
									<img src="images/Welsh_springer_spaniel_08203.jpg"  width='160', height='160'>
									<img src="images/Irish_red_and_white_setter_05763.jpg" width='160', height='160'>
									<figcaption>Brittany vs. Welsh Springer Spaniel vs. Irish Red & White Setter</figcaption>
								</figure></center>
								<!--
								<h4>Highlight</h4>
								<p>Separated they live in Bookmarksgrove right at the coast of the Semantics, a large language ocean. A small river named Duden flows by their place and supplies it with the necessary regelialia</p>
							-->
						</div>
						</div>
					</div>

					<div class="row rp-b">
						<div class="col-md-12 animate-box">
								<div class="fh5co-highlight right">

								<center><figure>

							<img src="images/American_water_spaniel_00625.jpg" width='200', height='200'>
								<img src="images/Curly-coated_retriever_03878.jpg"  width='170', height='200'>
								<figcaption>American Water Spaniel vs. Curly-Coated Retriever</figcaption>

							</figure></center>

							<center><figure>
							<img src="images/American_staffordshire_terrier_00559.jpg" width='180', height='180'>
								<img src="images/American_staffordshire_terrier_00615.jpg"  width='145', height='180'>
									<img src="images/American_staffordshire_terrier_00610.jpg"  width='145', height='180'>
								<figcaption> American Staffordshire Terrier</figcaption>
							</figure></center>

							<!--
							<blockquote>
								<p>&ldquo;She packed her seven versalia, put her initial into the belt and made herself on the way. When she reached the first hills of the Italic Mountains, she had a last view back on the skyline of her hometown Bookmarksgrove &rdquo; <cite>&mdash; Jean Smith</cite></p>
							</blockquote>
						-->
							</div>
						</div>
					</div>
<!--
					<div class="row rp-b">
						<div class="col-lg-6 col-md-12 animate-box">
							<figure>
								<img src="images/pic_1.jpg" alt="Free HTML5 Bootstrap Template by FREEHTML5.co" class="img-responsive">
								<figcaption>This is a sweet image caption. Far far away, behind the word mountains, far from the countries Vokalia and Consonantia</figcaption>
							</figure>
						</div>
						<div class="col-lg-6 col-md-12 cp-l animate-box">


							</div>
					</div>
				-->


					<div class="row">
						<div class="col-md-12 animate-box">
							<h2>Step 1: Dataset - Break dataset into Training, Testing, and Validation sets</h2>
							<p> I use the dog dataset from
									<a href="https://github.com/udacity/dog-project" target="_blank">Udacity</a>
								,which includes 133 dog breeds with 8,351 total dog images.
									The data are split into a training set (80%), a validation set (10%),
									and a test set (10%). The training set is used to determine the best-fit weights
									for the model, while the validation set allows me to check that the model complexity
									is appropriate, which means it won't be overfitting or underfitting.
									Finally, the test set is used only after deciding on the model parameters,
									to determine the overall accuracy of my model.

							<p>	The images below are samples of some of the dogs in my dataset. Note that these images
								are not uniform; many include multiple dogs, people, and other objects. Although it is easy
								for us as humans to separate the dogs in these images from everything else, it is not
								super-trivial to train a Neural Network to do the same. However, with a little work,
								 our model could figure out how!</p>
						</div>
						<div class="col-md-12 animate-box">
							<figure>
								<img src="images/display.png" alt="Free HTML5 Bootstrap Template by FREEHTML5.co" class="img-responsive">
							</figure>
						</div>
						<!--
						<div class="col-md-4 animate-box">
							<figure>
								<img src="images/pic_6.jpg" alt="Free HTML5 Bootstrap Template by FREEHTML5.co" class="img-responsive">
								<figcaption>This is a sweet image caption. Far far away, behind the word mountains, far from the countries Vokalia and Consonantia</figcaption>
							</figure>
						</div>
						<div class="col-md-4 animate-box">
							<figure>
								<img src="images/pic_7.jpg" alt="Free HTML5 Bootstrap Template by FREEHTML5.co" class="img-responsive">
								<figcaption>This is a sweet image caption. Far far away, behind the word mountains, far from the countries Vokalia and Consonantia</figcaption>
							</figure>
						</div>
					-->
						<div class="col-md-12 animate-box">
							<h2>Step 2: Create a Convolutional Neural Network (CNN) to classify Dog Breeds (from Scratch)</h2>
							<p>With 133 dog breeds in my sample, random chance says that guessing the correct breed has
								a probability of less than 1%. Thus, as long as the accuracy is higher than 1%,
								we can say that our model is <i>useful</i> (although maybe <b>not very useful</b>
								if it is not much higher than 1% :0)</p>

								<p>I am going to use <a href="https://keras.io/">Keras</a>
								, a python library used for deep learning that is particularly popular because
							it is easy to get up and running. </p>
							<h3>Step 2-1. Pre-process the Data (Rescale the images)</h3>
							<p> We will start by pre-processing the data. Keras requires us to input our images as a 4D array
								(aka a 4D tensor), with the shape</p>

								<center><p><i><b>(n_samples, n_rows, n_columns, n_channels)</b></i></p></center>

							<p> where `n_samples` corresponds to the total number of images (or samples),
							and `n_rows`, `n_columns`, correspond to the pixel number of rows and columns
								for each image, and `n_channels` corresponds to the number of RGB channels for each image,
							which is 3. </p>


								<p>I have a function that takes as input the path to a color image file and
									converts that image into the proper 4D tensor format to feed into our CNN. It
									converts the image information (pixel value) into numpy arrays, and resizes each
									array to make them all uniformly 224 x 224 pixels.  </p>
									<div class="col-md-12 animate-box">
									<center><figure>
										<img src="images/ex1/ex1.001.jpeg" width="90%">
									</figure></center>
								</div>
					<!--				<figure>
										<img src="images/ex1/ex1.002.jpeg">
									</figure> -->
									<p>Then, when split into 3
									color channels, each image is represented by a 3D array (224, 224, 3).</p>
									<div class="col-md-12 animate-box">
									<center><figure>
										<img src="images/ex1/ex1.002.jpeg" width="90%">
									</figure></center>
								</div>
									<p> Next,
									an extra dimension is added to the 3D array to allow for multiple images (samples)
									to be processed. Thus, the images are handled as 4D tensors. The returned tensor
									for a single image will always have the shape </p>

									<center><p> <i><b>(1, 224, 224, 3)</i></b> </p></center>

									<p>I have another function that takes an array of strings, with each string being
										the path to an image, as input to convert those images into a 4D tensor with
										the shape </p>

									<center><p> <i><b>(n_samples, 224, 224, 3)</i></b> </p></center>

								<p> Finally, I rescale the images by dividing every pixel in every image by 255,
									which changes the range of each image from 0-255 into 0-1.
							</p>
						</div>
					</div>
					<div class="col-md-12 animate-box">
						<h3>Step 2-2. Define the CNN model architecture</h3>
						<br>
						<h4> <b>Why CNNs?</b> </h4>
						<p>Regular Neural Networks use vectors for hidden layers, which don't represent the full images well.
							In particular, the fully connective layers of a regular neural network
							are less time efficient and easily lead to overfitting.
							For example, the scales of the images in our sample are a respectable 224 x 224,
							which means a hidden layer needs to include 224 x 224 x 3 = 150,528 weights.
							If we want to use multiple layers (as expected), the number of weights would add up quickly.</p>
							<p> On the other hand, CNNs are especially powerful when
								we must train on multi-dimensional data, such as images. CNNs consist of <i>locally</i> connected layers, which use far fewer
								 weights compared to the densly connected layers of a regular neural network.
								  The locally connected layers efficiently prevent overfitting
								  and allow us to easily understand the image data because they naturally handle
									two dimensional patterns.
								 	You can read more details
									<a href="http://cs231n.github.io/convolutional-networks/#overview" target="_blank">here</a>.
									<center><figure>
										<img src="images/neural_net2.jpeg" width="90%">
									</figure>
									<figure>
										<img src="images/cnn.jpeg" width="90%">
								<figcaption>(Top) Regular Neural Network
									(Bottom) CNN. (<b> Source :</b> http://cs231n.github.io/convolutional-networks/#overview)</figcaption></figure></center>
					</div>
					<div class="col-md-12 animate-box">
						<h4><b>Design of CNN</b></h4>
						<p>A CNN consists of multiple layers, which include
							convolutional, pooling, and some fully connected layers
							(also used in regular Neural Networks).</p>
							<h4>(1) Convolutional Layer</h4>
							<p>A convolutional Layer consists of locally connected nodes, meaning
								that the nodes are only connected to a small subset of the previous layers' nodes.
								To build a convolutional layer,
						we first select a width (column) and height (row) that defines a convolution filter.
						The filter is a matrix that can have its own characteristic pattern,
						and each convolutional layer will have the task of searching for its filters pattern
						in the image.

					To do this, we simply move the filter horizontally and vertically over the matrix of image pixels,
					and at each position the convolutional filter returns a numerical result that specifies
					whether its pattern was seen locally.
		 			The image below demonstates how a convolutional layer works.
					In practice, one will use many convolutional layers, each searching for its own unique pattern
					in the image, in order to identify complex structures. </p>
								<center><figure>
									<img src="images/ex1/ex1.003.jpeg" width="100%">
								</figure></center>
								<h4>(2) Pooling Layer</h4>
								<p>Recall that a convolutional layer is a stack of feature maps where we have
									one feature map for each filter.

									More filters means a larger stack, which means that the dimensionality of
									our convolutional layers can get quite large.
									Higher dimensionality means we will need to use more parameters,
									which can lead to overfitting. Therefore, we need a method to reduce this dimensionality
									by using <i>pooling layers</i> within a CNN.</p>

									<p>Generally, there are two popular choices for types of pooling layers.
										The first type is a <i>max pooling layer</i>.
										Max pooling layers take a stack of feature maps as input,
										and are constructed by finding the maximum value from a subset of pixels
										in the input layer.
										The second type of pooling layer is a <i>global average</i>. As the name implies,
										this pooling layer simply stores the average of all values
										in the input layer, rather than considering smaller windows.
										The global average pooling is a more extreme type of dimensionality reduction.
										</p>
									<center><figure>
										<img src="images/ex1/ex1.004.jpeg" width="100%">
									</figure></center>
									<h4><b>My CNN model architecture</b></h4>
									<p>I put three convolutional layers in my CNN, as the figure below shows.
										The first convolutional layer uses 16 filters, and in general it identifies
										only very broad patterns. The next convolutional layer has 32 filters,
										and the third uses 64 filters. These latter layers are used to
										find more complex shapes and patterns. After each convolutional layer is a
										max pooling layer used to reduce dimensionality, and after the third max pooling layer
										is a global average pooling layer. Finally, I use a fully connected layer with
										a softmax activation function that returns probabilities for each dog breed.
									</p>

					 <p>The table below shows the summary of my CNN model.</p>
					 <center><figure>
						 <img src="images/sample_cnn.png" width="100%">
					 </figure></center>

					 <p>Note that the number of parameters is large, but not prohibitively large
						  (regular neural networks could easily reach millions of parameters).
						  </p>
					 </div>
					 <div class="col-md-12 animate-box">
						 <h3>Step 2-3. Loss function </h3>
						 <br>
<p>
						 After designing the CNN model, we need to specify a loss function
						 so that we can quantify the model accuracy.

						 Since we are constructing a multiclass classifier,
						 we will use <i>categorical cross-entropy loss</i>. </p>
						 <p>
						 This loss function returns a numerical
						 value that is lower if the model predicts
						 the true label (in this case, the correct dog breed).
							As with other classification tasks in machine learning,
							we want to minimize the loss function to train our model
							to give us the highest accuracy possible.

						 The true labels are one-hot encoded, and each label is a vector
						 with 133 entries.
						  The model outputs a vector having 133 entries,
							where each entry corresponds to the probability of that dog breed. </p>
							<center><figure>
								<img src="images/skier_awesome.png" width="100%">
								<figcaption>

									The illustration of Error loss function.
									The goal is to minimize the error by seeking parameters (weights)
									that minimize the loss function.
								 (Source: modified from Udacity Machine Learning Nano Degree image)</figcaption>
							</figure></center>
							<p>
						 For example, let's consider the image with a Welsh Springer Spaniel dog.
						 Our model predicts that there is a <i>Brittany</i> in the image
						 with a 90% propability and <i>Welsh Springer Spaniel</i> in the image with
						 a 10% probability. The categorical cross-entropy loss checks
						 the true label vector (with only Welsh Springer Spaniel selected) against
						 the prediction vector (which has 90% chances of Brittany and with only
						 10% chance of Welsh Springer Spaniel),
						 and returns a high value for the loss. The model then
						 adjusts the weights, and if the prediction changes to favor the true
						 label more, then the loss function decreases. Eventually,
						 if our model is good enough, we would find at the end of our training
						 that it correctly identifies most dog breeds.

							<h3>Step 2-4. Train the model</h3>
							<br>
							<p>In training the model, I modify the weignts to improve the predictions.
							I chose to train the model for 5 epochs, and I saved the weights that
							correspond to the highest validation accuracy.
							This process took around 20 min on my laptop.
							Expect it to take more time if you want to train your model with a larger
							number of epochs.</p>

							<h3>Step 2-5. Load the model and calculate accuracy on test set</h3>
							<br>
							<p>Once the training has finished, I load the saved model weights and
							calculate the classification accuracy on the test set.
							The accuracy from my CNN model
							is around 2.4%, which is just a little better than random guessing,
							and it's not great.</p>

					</div>
					<div class="col-md-12 animate-box">
						<h2>Step 3: Using Transfer Learning</h2>

						<h3>Why Transfer Learning?</h3>
						<br>
						<p>As we have seen from the result above, using a CNN model from scratch
							does not often lead to a satisfying result. The reason our CNN model was
							not very good at accurately identifying dog breeds is that it lacked sufficient
							complexity to identify the wide variety of objects in the images. If your model
							can't tell the difference between a dog, a person, and a tree, then it certainly
							will not be very good at distinguishing between breeds of dogs. But, what if we
							could take the foundation of a sufficiently complex neural network that <i>has</i>
							already been trained to identify objects like dogs and people, and build off of that
							existing foundation?

						<p>This is the basic idea behind transfer learning, which has proven to be a fruitful
							approach to developing CNNs to perform complex tasks. We can take a CNN model which has
							already been trained on image data, and use its foundational architecture to apply to
							a similar problem that we want to solve. If the model has been trained to
							identify the right kind of object (in this case, dogs), then it can very easily be
							repurposed for our classification problem. This will save us a great deal of time
							and effort, because there's no need to reinvent the wheel every time we need to solve a
							slightly different problem.

				</p>
				<center><figure>
					<img src="images/transfer-learning.jpeg" >
				</figure></center>
				<p>There are existing CNN models that have been pre-trained on
					<a href="http://www.image-net.org/">ImageNet</a>,
					a database containing nearly 15 million images depicting a vast array
					of objects and scenes. The models listed below are available in Keras, and
					they have all been pre-trained on ImageNet. You can click each link to find
					out more about how you can apply the model you like. </p>
				<ul>
					<li><a href="https://keras.io/applications/#vgg16">VGG16</a></li>
					<li><a href="https://keras.io/applications/#vgg19">VGG19</a></li>
					<li><a href="https://keras.io/applications/#inceptionv3">InceptionV3</a></li>
					<li><a href="https://keras.io/applications/#xception">Xception</a></li>
				</ul>
				<br>
				<h3>How does it work?</h3>
				<br>
				<p>
				If you take a CNN that has been pre-trained on the ImageNet database, then
				you can be reasonably confident that your CNN has already learned how to
				distinguish between the 1,000 different categories of objects that are
				represented by ImageNet. Most of those categories are animals, fruits,
				vegetables, and other everyday objects.</p>

				<p>You might now be asking yourself: "If I want to use a CNN for a
				classification task involving something that is not found in the ImageNet
				database, is it still relevant or beneficial to use that pre-trained CNN knowledge?"
				The answer is most definitely yes.
			</p>
			<center><figure>
				<img src="images/bN2iA.png" width="80%">
				<figcaption>A example of representative layers of a CNN.
					Note that earlier stages try to find simpler features. (Source :
				http://cs231n.stanford.edu/)</figcaption>
			</figure></center>
			<p> The convolutional filters in a trained CNN are arranged in a hierarchy.
				The filters in the first layer often only detect edges or blobs of color.
				The second layer might detect circles, stripes, and rectangles.
				There early features are very generalized, and are typically useful for
				analyzing any image in any dataset. The filters in the final convolutional
				layers are much more specific.

				If there were birds in the training data set, then there are filters that can detect birds.
				If there were cars or bicycle, there are filters to detect wheels and so on.
				We will see that it's then useful to remove the final layers of the pre-trained network that
				are very specific to a particular training data set, while keeping the earlier layers
				when we do transfer learning.
				Then, we simply add one or two more layers and train only those final layers
				on our particular classification task.
				This is one way to do transfer learning, but your preferred method will depend on both
				the size of your dataset and the level of similarity it shares with the ImageNet
				database.
			</p>

			<p>Note that the transfer learning technique will work well
			if your data set is relatively small and very similar to ImageNet.

			Our dog data set is appropriately small and it has significant overlap with
			a subset of the ImageNet categories, which makes it ideal for this application! :-)
		</p>

			<center><figure>
				<img src="images/ex1/ex1.005.jpeg" width="100%">
			</figure></center>

			 <p>First, we slice off the end of
			the network (the layers highlighted by black boxes
			in the figure above) and add a new classification layer with 133 nodes (one for each of
			our output labels). We then train only the weights in this layer, freezing all
			the weights in the earlier layers.
			We can take an image, pass it through the pre-trained model, and stop at the last
		VGG16/VGG19 max pooling layer. This turns each image into another 3D array,
	which we can then save as part of a new dataset.

	Then, when we go to train our network, we will use this new data set as the inputs
	and our network in Keras will have two layers: an input and an output layer.
	Thus, our model needs to take as input a 7 x 7 x 512 array for each image.
  That will result in a lot of parameters, so I also do some dimensionality reduction
through a global average pooling layer. The figures below show my CNN architecture
when using VGG16 and VGG19 as the pre-trained foundation.</p>

<center><figure>
	<img src="images/ex1/ex1.006.jpeg" width="100%">
</figure></center>

<p>For this project, I used VGG16/19, InceptionV3, and Xception by adding global average pooling
	layer and fully connected layer with softmax activation functions. Finally, I compiled the models
	with the same loss function described earlier and I check the accuracies against the test set.
	Below are the resulting accuracies for each model. </p>
	<ul>
		<li>VGG16 - 45%</li>
		<li>VGG19 - 48%</li>
		<li>InceptionV3 - 80%</a></li>
		<li>Xception - 84%</a></li>
	</ul>
				<p>Using Xception results in the highest accuracy. Thus, I want to apply the model
					using Xception to predict dog breed! 84% is not bad, right? :-)</p>
					<center><figure>
						<img src="images/misun.png" width="70%" style="border: 3px solid #E09938;">
						<figcaption>This is a photo of my dog! Yeah. The model got it!!</figcaption>
						</figure></center>
				</div>
				<div class="col-md-12 animate-box">
					<h2>Step 4: Apply the Model to predict dog breed</h2>

								<h3>Mis-predicted dogs</h3>

					<p> When I apply my model to predict dog breeds,
					I failed to correctly predict dog breeds for 130 out of 836 test images.
				The images below show examples of the dogs my model failed to correctly guess.
			The very left images are the input images, with the correct breed label printed in
			a white box, while the images to the right show the the dog breeds my model
			predicted for that dog, sorted by the probabilities assigned by the model.

			Note that many of the dogs my model fails to accurately predict seem not to be
			easy to distinguish even with human eyes.
			</p>
					<center><figure>
						<img src="images/result1.png" width="90%">
						<figcaption>Examples of dogs for which my model fails to find the correct breed
							(with an incorrect prediction probability of more than 0.8)
						</figcaption>
						</figure></center>

						<p> The images show examples of incorrect guesses again, but this time
							where the prediction probability is less than 0.8. Note that it's even
							more apparent in these cases that it's really hard for human eyes
						to find the correct breed. </p>
						<center><figure>
						<img src="images/result2.png" width="90%">
						<figcaption>Examples of dogs for which my model fails to find the correct dog breed
							(with a prediction probability of less than 0.8)
						</figcaption>
					</figure></center>
						<p>Based on the fact that my model accuracy is higher than 84%, overall
							it seems to work pretty well. For example, in the case of the Brittany dogs I mentioned at
						the very begining, our model predicts the breed correctly for 4 out of 6 test images.
						This is pretty impressive, considering that their appearance looks very similar
						to other breeds, such as Welsh Springer Spaniel or Irish Red & White setter.
						If we have enough GPU memory, I believe I could build an even better model
						to classify dog breeds after testing with different model architectures.
						Feel free to train your own model with transfer learning and find an even
						better model to categorize dog breeds!</p>


									<h3>How about mixed breeds?</h3>
									<p>
									If we test mixed breeds on our model, can it predict <i>at least</i>
									one of dog breeds for those with mixed breeds?
									I tested it with several mixed-breed dogs.
									Out of 7 mixed breeds I used for the testing,
									only 2 dogs are correctly predicted by the model.
									The figures below show a subset of the images of mixed breed dogs.</p>
									<br>
									<center><figure>
										<img src="images/ex1/ex1.007.jpeg" width="100%">
										<figcaption> Australian Shepherd + Pomeranian breed.
											This is predicted as Alaskan Malamute by my model.
										</figcaption>
										</figure></center>
										<br>
										<br>
										<center><figure>
											<img src="images/ex1/ex1.008.jpeg" width="100%">
											<figcaption> Golden Retriever + Poodle.
												This is predicted as Otterhound by my model.
											</figcaption>
											</figure></center>
											<br>
											<br>
											<center><figure>
												<img src="images/ex1/ex1.009.jpeg" width="100%">
												<figcaption> Doberman + Boxer + Labrador.
													This is predicted as Great Swiss Mountain Dog by my model.
												</figcaption>
												</figure></center>
												<br>
												<br>
												<center><figure>
													<img src="images/ex1/ex1.010.jpeg" width="100%">
													<figcaption> Bulldog + Dalmatian.
														This is predicted as Dalmatian by my model.
													</figcaption>
													</figure></center>
													<br>
										<p> The last figure was correctively predicted.
										However, the dog (Bulldog + Dalmatian) looks very close to
										being a pure dalmatian, doesn't it?
									Thus, I guess it is really hard to predict mixed-breed dogs, although all of
								the natural breeds for the mixed breeds are trained by my model.
							In order to categorize mixed-breed dogs, I think we need to put the specific mixed-breed
							cases as new entries in our training and validation data if we want
							to do a better job of predicting those mixed-breed cases.
					</p>
				</div>
				<div class="col-md-12 animate-box">
					<h2>Summary and Future work.</h2>
					<p>I trained more than 8,000 dog images with 133 different breeds to predict their dog breed automatically
						using artificial intelligence.
						I designed a CNN model from scratch, and found that the result was not impressive (a little higher than 2%).
					Next, I used several different transfer learning models, including VGG, Inception, and Xception,
					which finally led to a model with 85% accuracy on the test set with the Xception bottleneck feature.
					The result seems to be pretty good, at least similar to (or even better) than human eyes.
					There are still problems with categorizing mixed dog breeds, so I believe
					we would need to train a model on a wider range of dog breeds, pure and mixed,
					in order to build a more generalized neural network to identify the breed(s)
					of any dog.
					<br>
					<center><figure>
						<img src="images/poodle.jpg" width="50%">
						</figure></center>
				</div>
				<div class="col-md-12 animate-box">
					<h3>Reference</h3>
					<ul>
						<li><a href="https://www.udacity.com/">Udacity Machine Learning Nanodegree program</a></li>
						<li><a href="http://cs231n.github.io/">The Lecutre Note of CS231n Convolutional Neural Nerworks for Visual Recognition by Stanford</a></li>
						<li><a href="https://www.slideshare.net/matsukenbook/deep-learning-chap6-convolutional-neural-net">
							Deep Learning: Chap 6. Convolutional Neural Net by Ken'ichi Matsui</a></li>
						<li><a href="https://elitedatascience.com/keras-tutorial-deep-learning-in-python">
							Keras Tutorial in EliteDataScience</a></li>
							<li><a href="https://www.pyimagesearch.com/2017/03/20/imagenet-vggnet-resnet-inception-xception-keras/">
								ImageNet: VGGNet, ResNet, Inception, and Xception with Keras by Adrian Rosebrock</a></li>
							<li><a href="https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/">
								An Intuitive Explanation of Convolutional Neural Networks by ujjwalkarn</a></li>
								<li><a href="https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/">
									Transfer learning & The art of using Pre-trained Models in Deep Learning by Dishashree Gupta</a></li>
					</ul>

				</div>
			</article>
		</div>
	</div>

	<footer id="fh5co-footer">
		<p><small>&copy; 2016. Magazine Free HTML5. All Rights Reserverd. <br> Designed by <a href="http://freehtml5.co" target="_blank">FREEHTML5.co</a>  Demo Images: <a href="http://unsplash.com/" target="_blank">Unsplash</a></small></p>
	</footer>

	<!-- jQuery -->
	<script src="js/jquery.min.js"></script>
	<!-- jQuery Easing -->
	<script src="js/jquery.easing.1.3.js"></script>
	<!-- Bootstrap -->
	<script src="js/bootstrap.min.js"></script>
	<!-- Waypoints -->
	<script src="js/jquery.waypoints.min.js"></script>
	<!-- Main JS -->
	<script src="js/main.js"></script>

	</body>
</html>
